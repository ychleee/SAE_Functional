{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Conditional SAE Analysis\n",
    "\n",
    "This notebook prepares synthetic and natural datasets for training SAEs on conditional features.\n",
    "Run this locally before uploading to Colab for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Import our data utilities\n",
    "from src.data_utils import (\n",
    "    ConditionalDatasetGenerator,\n",
    "    create_minimal_test_set,\n",
    "    save_dataset,\n",
    "    load_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../configs/training_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Number of samples: {config['data']['n_samples']}\")\n",
    "print(f\"  Data directory: {config['paths']['data_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator\n",
    "generator = ConditionalDatasetGenerator(seed=42)\n",
    "\n",
    "# Generate dataset\n",
    "print(f\"Generating {config['data']['n_samples']} sentences...\")\n",
    "df = generator.generate_dataset(n_samples=config['data']['n_samples'])\n",
    "\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Total sentences: {len(df)}\")\n",
    "print(f\"  Conditionals: {df['has_conditional'].sum()}\")\n",
    "print(f\"  Controls: {(~df['has_conditional']).sum()}\")\n",
    "print(f\"\\nSentence types:\")\n",
    "print(df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample sentences\n",
    "print(\"Sample sentences by type:\\n\")\n",
    "for sentence_type in df['type'].unique():\n",
    "    print(f\"{sentence_type.upper()}:\")\n",
    "    samples = df[df['type'] == sentence_type]['text'].head(3)\n",
    "    for i, text in enumerate(samples, 1):\n",
    "        print(f\"  {i}. {text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Test Sets for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimal test set for logical inference\n",
    "test_df = create_minimal_test_set()\n",
    "\n",
    "print(\"Test cases for inference:\")\n",
    "for _, row in test_df.iterrows():\n",
    "    print(f\"\\nType: {row['type']}\")\n",
    "    print(f\"  Premise: {row['premise']}\")\n",
    "    print(f\"  Fact: {row['fact']}\")\n",
    "    print(f\"  Conclusion: {row['conclusion']}\")\n",
    "    print(f\"  Valid: {row['valid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Dataset Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"Text statistics:\")\n",
    "print(df[['text_length', 'word_count']].describe())\n",
    "\n",
    "# Plot distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df['text_length'], bins=20, edgecolor='black')\n",
    "axes[0].set_xlabel('Character Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Text Length Distribution')\n",
    "\n",
    "axes[1].hist(df['word_count'], bins=15, edgecolor='black')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Word Count Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze keyword frequencies\n",
    "keywords = ['if', 'then', 'and', 'or', 'not', 'because', 'when']\n",
    "\n",
    "for keyword in keywords:\n",
    "    df[f'has_{keyword}'] = df['text'].str.lower().str.contains(keyword)\n",
    "\n",
    "keyword_stats = pd.DataFrame({\n",
    "    'keyword': keywords,\n",
    "    'count': [df[f'has_{keyword}'].sum() for keyword in keywords],\n",
    "    'percentage': [100 * df[f'has_{keyword}'].mean() for keyword in keywords]\n",
    "})\n",
    "\n",
    "print(\"Keyword frequencies:\")\n",
    "print(keyword_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = Path('..') / config['paths']['data_dir']\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save main dataset\n",
    "main_dataset_path = data_dir / 'conditionals_dataset.csv'\n",
    "save_dataset(df, str(main_dataset_path))\n",
    "print(f\"Main dataset saved to: {main_dataset_path}\")\n",
    "\n",
    "# Save test set\n",
    "test_dataset_path = data_dir / 'inference_test_set.csv'\n",
    "save_dataset(test_df, str(test_dataset_path))\n",
    "print(f\"Test set saved to: {test_dataset_path}\")\n",
    "\n",
    "# Also save as JSON for easier inspection\n",
    "json_path = data_dir / 'conditionals_dataset.json'\n",
    "save_dataset(df.head(100), str(json_path))  # Save first 100 for inspection\n",
    "print(f\"Sample saved to JSON: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Extended Datasets (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a focused dataset with only simple conditionals\n",
    "simple_conditionals = df[df['type'] == 'simple_conditional']\n",
    "print(f\"Simple conditionals: {len(simple_conditionals)}\")\n",
    "\n",
    "# Create a balanced dataset\n",
    "n_per_type = df['has_conditional'].value_counts().min()\n",
    "balanced_df = pd.concat([\n",
    "    df[df['has_conditional']].sample(n=n_per_type, random_state=42),\n",
    "    df[~df['has_conditional']].sample(n=n_per_type, random_state=42)\n",
    "]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Balanced dataset: {len(balanced_df)} (50% conditional, 50% control)\")\n",
    "\n",
    "# Save additional datasets\n",
    "save_dataset(simple_conditionals, str(data_dir / 'simple_conditionals.csv'))\n",
    "save_dataset(balanced_df, str(data_dir / 'balanced_dataset.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare for GitHub Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check file sizes\n",
    "import os\n",
    "\n",
    "print(\"File sizes:\")\n",
    "for file in data_dir.glob('*.csv'):\n",
    "    size_kb = os.path.getsize(file) / 1024\n",
    "    print(f\"  {file.name}: {size_kb:.1f} KB\")\n",
    "\n",
    "print(\"\\nThese files are small enough to commit to GitHub.\")\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. git add -A\")\n",
    "print(\"  2. git commit -m 'Add prepared datasets'\")\n",
    "print(\"  3. git push origin main\")\n",
    "print(\"  4. Upload notebook 02_sae_training.ipynb to Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully created:\n",
    "1. **Main dataset** with various types of conditionals and control sentences\n",
    "2. **Test set** for logical inference evaluation\n",
    "3. **Balanced dataset** for controlled experiments\n",
    "4. **Simple conditionals** subset for focused analysis\n",
    "\n",
    "The data is ready to be pushed to GitHub and used in the Colab training notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}