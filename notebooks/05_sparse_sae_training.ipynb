{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Truly Sparse SAE Training for Feature Overlap Analysis\n",
        "\n",
        "This notebook trains a properly sparse autoencoder with stronger sparsity constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SparseSAE(nn.Module):\n",
        "    \"\"\"Sparse Autoencoder with L1 penalty and top-k activation.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim=2048, sparsity_coeff=0.1, top_k=50):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)\n",
        "        self.sparsity_coeff = sparsity_coeff  # Increased from 0.01\n",
        "        self.top_k = top_k  # Only keep top k features active\n",
        "        \n",
        "        # Tie weights\n",
        "        self.decoder.weight = nn.Parameter(self.encoder.weight.t())\n",
        "        \n",
        "        # Better initialization for sparsity\n",
        "        nn.init.xavier_uniform_(self.encoder.weight, gain=0.1)\n",
        "        nn.init.zeros_(self.encoder.bias)\n",
        "    \n",
        "    def encode(self, x):\n",
        "        h = torch.relu(self.encoder(x))\n",
        "        \n",
        "        # Apply top-k sparsity (only during training)\n",
        "        if self.training and self.top_k is not None:\n",
        "            # Keep only top k activations per sample\n",
        "            topk_vals, topk_idx = torch.topk(h, self.top_k, dim=1)\n",
        "            mask = torch.zeros_like(h)\n",
        "            mask.scatter_(1, topk_idx, 1)\n",
        "            h = h * mask\n",
        "        \n",
        "        return h\n",
        "    \n",
        "    def forward(self, x):\n",
        "        code = self.encode(x)\n",
        "        recon = self.decoder(code)\n",
        "        return recon, code\n",
        "    \n",
        "    def loss(self, x, beta_warm=1.0):\n",
        "        recon, code = self.forward(x)\n",
        "        \n",
        "        # Reconstruction loss\n",
        "        recon_loss = nn.functional.mse_loss(recon, x)\n",
        "        \n",
        "        # L1 sparsity penalty (with warm-up)\n",
        "        l1_loss = code.abs().mean()\n",
        "        \n",
        "        # L0 penalty (number of active features)\n",
        "        l0_loss = (code > 0).float().mean()\n",
        "        \n",
        "        # Combined loss\n",
        "        total_loss = recon_loss + beta_warm * self.sparsity_coeff * l1_loss\n",
        "        \n",
        "        return total_loss, {\n",
        "            'total': total_loss.item(),\n",
        "            'recon': recon_loss.item(),\n",
        "            'l1': l1_loss.item(),\n",
        "            'l0': l0_loss.item(),\n",
        "            'active_features': (code > 0).float().sum(dim=1).mean().item()\n",
        "        }\n",
        "\n",
        "print(\"SparseSAE class defined with:\")\n",
        "print(\"- Stronger sparsity coefficient (0.1 vs 0.01)\")\n",
        "print(\"- Top-k activation constraint\")\n",
        "print(\"- Better initialization for sparsity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training with better sparsity\n",
        "def train_sparse_sae(activations, device='cuda', \n",
        "                     hidden_dim=2048, \n",
        "                     sparsity_coeff=0.1,  # 10x stronger\n",
        "                     top_k=30,  # Only 30 active features max\n",
        "                     epochs=200,  # More epochs\n",
        "                     lr=0.0005):  # Lower learning rate\n",
        "    \n",
        "    input_dim = activations.shape[1]\n",
        "    sae = SparseSAE(input_dim, hidden_dim, sparsity_coeff, top_k).to(device)\n",
        "    \n",
        "    optimizer = optim.Adam(sae.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "    \n",
        "    acts_tensor = activations.to(device)\n",
        "    \n",
        "    print(f\"Training Sparse SAE: {input_dim} → {hidden_dim} features\")\n",
        "    print(f\"Sparsity: L1 coefficient={sparsity_coeff}, Top-k={top_k}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        # Warm up sparsity penalty\n",
        "        beta_warm = min(1.0, epoch / 50)  # Warm up over 50 epochs\n",
        "        \n",
        "        # Forward pass\n",
        "        loss, metrics = sae.loss(acts_tensor, beta_warm)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(sae.parameters(), 1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        losses.append(metrics)\n",
        "        \n",
        "        if epoch % 40 == 0:\n",
        "            print(f\"Epoch {epoch:3d}: Loss={metrics['total']:.4f} \"\n",
        "                  f\"(R:{metrics['recon']:.4f} L1:{metrics['l1']:.4f}) \"\n",
        "                  f\"Active={metrics['active_features']:.1f}\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(f\"✓ Final active features per sample: {metrics['active_features']:.1f}\")\n",
        "    print(f\"✓ Final L0 (fraction active): {metrics['l0']:.4f}\")\n",
        "    \n",
        "    return sae, losses\n",
        "\n",
        "# This will be called after loading activations\n",
        "print(\"Training function ready. Will enforce true sparsity.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Changes for Proper Sparsity:\n",
        "\n",
        "1. **Sparsity coefficient: 0.1** (was 0.01) - 10x stronger L1 penalty\n",
        "2. **Top-k constraint: 30** - Maximum 30 features can be active (out of 2048)\n",
        "3. **Warm-up schedule** - Gradually increase sparsity penalty\n",
        "4. **Better initialization** - Xavier with gain=0.1 for sparser start\n",
        "5. **200 epochs** - More training for features to specialize\n",
        "\n",
        "Expected results:\n",
        "- ~10-30 active features per sentence (not 140!)\n",
        "- Clearer separation between conditional-specific and quantifier-specific features\n",
        "- More meaningful overlap percentage"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}