{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Training on Conditional Features (Colab GPU)\n",
    "\n",
    "This notebook should be run on Google Colab with GPU enabled.\n",
    "\n",
    "**Steps:**\n",
    "1. Clone repository from GitHub\n",
    "2. Install dependencies\n",
    "3. Load prepared dataset\n",
    "4. Extract model activations\n",
    "5. Train Sparse Autoencoder\n",
    "6. Save results to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import os\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"Not in Colab - make sure to upload this to Colab for GPU access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repository\nif IN_COLAB:\n    !git clone https://github.com/ychleee/SAE_Functional.git\n    %cd SAE_Functional\nelse:\n    # If running locally for testing\n    import sys\n    sys.path.append('..')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "if IN_COLAB:\n",
    "    !pip install -q -r requirements_colab.txt\n",
    "    print(\"Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Fix NumPy compatibility issues\nimport subprocess\nimport sys\n\n# Restart runtime after installing to avoid conflicts\nif IN_COLAB:\n    # Uninstall and reinstall numpy/pandas with compatible versions\n    !pip uninstall -y numpy pandas numexpr\n    !pip install numpy==1.23.5 pandas==1.5.3\n    \n    print(\"NumPy and Pandas reinstalled with compatible versions\")\n    print(\"You may need to restart the runtime: Runtime -> Restart runtime\")\n    print(\"Then run from the next cell onwards (skip the setup cells)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving results\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create results directory in Drive\n",
    "    DRIVE_DIR = '/content/drive/MyDrive/sae_conditionals_results'\n",
    "    !mkdir -p {DRIVE_DIR}\n",
    "    print(f\"Results will be saved to: {DRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load configuration\n",
    "with open('configs/training_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"SAE features: {config['sae']['hidden_dim']}\")\n",
    "print(f\"Sparsity coefficient: {config['sae']['sparsity_coeff']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared dataset\n",
    "# Note: You need to have run 01_data_preparation.ipynb first and pushed to GitHub\n",
    "data_path = Path(config['paths']['data_dir']) / 'conditionals_dataset.csv'\n",
    "\n",
    "if data_path.exists():\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} sentences\")\n",
    "    print(f\"Conditionals: {df['has_conditional'].sum()}\")\n",
    "    print(f\"Controls: {(~df['has_conditional']).sum()}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {data_path}\")\n",
    "    print(\"Creating a small synthetic dataset for testing...\")\n",
    "    \n",
    "    # Import data generation utilities\n",
    "    from src.data_utils import ConditionalDatasetGenerator\n",
    "    \n",
    "    generator = ConditionalDatasetGenerator()\n",
    "    df = generator.generate_dataset(n_samples=500)\n",
    "    print(f\"Generated {len(df)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Model Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.activation_extraction import ActivationExtractor, prepare_activations_for_sae\n",
    "\n",
    "# Initialize extractor\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "extractor = ActivationExtractor(\n",
    "    model_name=config['model']['name'],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for the dataset\n",
    "print(\"Extracting activations...\")\n",
    "activations_dict = extractor.extract_dataset_activations(\n",
    "    df,\n",
    "    text_column='text',\n",
    "    layer_idx=config['model']['layer_idx'],\n",
    "    batch_size=config['data']['batch_size'],\n",
    "    max_samples=config['data']['n_samples']\n",
    ")\n",
    "\n",
    "print(f\"Activations shape: {activations_dict['shape']}\")\n",
    "print(f\"Extracted from layer: {activations_dict['layer_idx']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare activations for SAE training\n",
    "pooled_activations = prepare_activations_for_sae(\n",
    "    activations_dict,\n",
    "    pool_method=config['data']['pool_method']\n",
    ")\n",
    "\n",
    "print(f\"Pooled activations shape: {pooled_activations.shape}\")\n",
    "print(f\"Input dimension for SAE: {pooled_activations.shape[1]}\")\n",
    "\n",
    "# Update config with actual input dimension\n",
    "actual_input_dim = pooled_activations.shape[1]\n",
    "config['sae']['input_dim'] = actual_input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation\n",
    "n_train = int(len(pooled_activations) * config['data']['train_val_split'])\n",
    "train_data = pooled_activations[:n_train]\n",
    "val_data = pooled_activations[n_train:]\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sae_training import SparseAutoencoder, SAETrainer\n",
    "\n",
    "# Create SAE model\n",
    "sae_model = SparseAutoencoder(\n",
    "    input_dim=config['sae']['input_dim'],\n",
    "    hidden_dim=config['sae']['hidden_dim'],\n",
    "    sparsity_coeff=config['sae']['sparsity_coeff'],\n",
    "    use_bias=config['sae']['use_bias']\n",
    ")\n",
    "\n",
    "print(f\"SAE Model:\")\n",
    "print(f\"  Input dimension: {config['sae']['input_dim']}\")\n",
    "print(f\"  Hidden dimension: {config['sae']['hidden_dim']}\")\n",
    "print(f\"  Sparsity coefficient: {config['sae']['sparsity_coeff']}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in sae_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SAETrainer(sae_model, device=device)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "history = trainer.train(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    epochs=config['training']['epochs'],\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    learning_rate=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay'],\n",
    "    patience=config['training']['patience'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = trainer.plot_history()\n",
    "plt.show()\n",
    "\n",
    "# Final metrics\n",
    "final_train = history['train'][-1]\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(f\"  Total Loss: {final_train['total_loss']:.4f}\")\n",
    "print(f\"  Reconstruction Loss: {final_train['recon_loss']:.4f}\")\n",
    "print(f\"  Sparsity Loss: {final_train['sparsity_loss']:.4f}\")\n",
    "print(f\"  Active Features: {final_train['active_features']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Learned Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sae_training import analyze_features\n",
    "\n",
    "# Analyze which features correspond to conditionals\n",
    "analysis = analyze_features(\n",
    "    model=sae_model,\n",
    "    activations=pooled_activations,\n",
    "    metadata=activations_dict['metadata'],\n",
    "    top_k=config['analysis']['top_k_features']\n",
    ")\n",
    "\n",
    "print(\"Top Conditional Features:\")\n",
    "for i, (feat_idx, score) in enumerate(zip(\n",
    "    analysis['conditional_features'][:5],\n",
    "    analysis['conditional_scores'][:5]\n",
    ")):\n",
    "    print(f\"  {i+1}. Feature {feat_idx}: differential score = {score:.3f}\")\n",
    "\n",
    "print(\"\\nFeature Statistics:\")\n",
    "for key, value in analysis['feature_stats'].items():\n",
    "    print(f\"  {key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_analysis import FeatureInterpreter, create_feature_report\n",
    "\n",
    "# Create feature interpreter\n",
    "interpreter = FeatureInterpreter(\n",
    "    sae_model=sae_model,\n",
    "    activations_dict={'activations': pooled_activations, 'metadata': activations_dict['metadata']},\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Analyze conditional features\n",
    "conditional_analysis = interpreter.analyze_conditional_features(top_k=10)\n",
    "\n",
    "print(\"\\nConditional Feature Analysis:\")\n",
    "print(f\"Top 'if' features: {conditional_analysis['if_features']['indices'][:5]}\")\n",
    "print(f\"Top 'then' features: {conditional_analysis['then_features']['indices'][:5]}\")\n",
    "print(f\"Top conditional features: {conditional_analysis['conditional_features']['indices'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate report for top conditional feature\n",
    "top_feature = analysis['conditional_features'][0]\n",
    "report = create_feature_report(interpreter, top_feature, n_examples=5)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Create timestamp for this run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_dir = f\"{DRIVE_DIR}/run_{timestamp}\"\n",
    "    !mkdir -p {run_dir}\n",
    "    \n",
    "    # Save trained model\n",
    "    model_path = f\"{run_dir}/sae_model.pt\"\n",
    "    torch.save({\n",
    "        'model_state_dict': sae_model.state_dict(),\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "        'analysis': analysis\n",
    "    }, model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save activations\n",
    "    activations_path = f\"{run_dir}/activations.pt\"\n",
    "    torch.save(activations_dict, activations_path)\n",
    "    print(f\"Activations saved to: {activations_path}\")\n",
    "    \n",
    "    # Save analysis results\n",
    "    analysis_path = f\"{run_dir}/analysis.json\"\n",
    "    with open(analysis_path, 'w') as f:\n",
    "        json.dump(conditional_analysis, f, indent=2)\n",
    "    print(f\"Analysis saved to: {analysis_path}\")\n",
    "    \n",
    "    # Save training plot\n",
    "    fig = trainer.plot_history()\n",
    "    fig.savefig(f\"{run_dir}/training_history.png\")\n",
    "    print(f\"Training plot saved to: {run_dir}/training_history.png\")\n",
    "    \n",
    "    print(f\"\\nAll results saved to: {run_dir}\")\n",
    "else:\n",
    "    print(\"Not in Colab - results not saved to Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Results for Local Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for downloading results\n",
    "if IN_COLAB:\n",
    "    print(\"To download results to your local machine:\")\n",
    "    print(f\"1. Navigate to Google Drive: {DRIVE_DIR}\")\n",
    "    print(\"2. Download the latest run_TIMESTAMP folder\")\n",
    "    print(\"3. Place in your local project's models/checkpoints directory\")\n",
    "    print(\"\\nAlternatively, use rclone or Google Drive sync on your local machine\")\n",
    "else:\n",
    "    print(\"Upload this notebook to Colab to train with GPU and save results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Download results** from Google Drive to your local machine\n",
    "2. **Run analysis notebook** (03_analysis.ipynb) locally to:\n",
    "   - Perform detailed feature interpretation\n",
    "   - Create visualizations\n",
    "   - Test causal interventions\n",
    "3. **Experiment with hyperparameters** by modifying training_config.yaml\n",
    "4. **Try different models** (Pythia-410M, GPT-2-medium) for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}